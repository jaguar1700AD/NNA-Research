{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements 1 level cache (No backward pass needed so not implemented; only accuracy determination required)\n",
    "# Weight sharing not included\n",
    "\n",
    "# Cache.approx just updates W, A values according to hits and misses and does not compute the result. Actual result computation\n",
    "# is done at the end in parallel by doing updated_W * updated_A\n",
    "# Current - LRU policy replacement. Future - May use LRU + count-used based policy\n",
    "\n",
    "# Find out how hdw simulation is done in papers. They ofcourse don't actually build the hdw, they just simulate it. But, \n",
    "# they are still able to find the accuracy value by running the program over their hdw simulation\n",
    "\n",
    "# To be added (and be performed in this order):\n",
    "# Network Pruning\n",
    "# Wt sharing\n",
    "# Bit Masking (both weights and activations; may try retraining also for both) / Pytorch trained 8 bit int quantization\n",
    "# May merge weight bit masking in 3rd step with 2nd step \n",
    "\n",
    "# Find out exactly how 1000 caches will be used in hdw. They all have the same data ? If miss occurs in one cache, access \n",
    "# to all caches will be stopped ? Using the parallelism of 1000 caches to process misses in parallel can really speed up\n",
    "# the simulation. The sequential processing of misses is the current bottleneck\n",
    "\n",
    "# Speed Up methods:\n",
    "# Prune W and A in initial step to be able to increase batch size (Currently, very difficult to vectorize. Basically, hard\n",
    "# to get equal size search ranges for each unique weight in the new weight-activation pairs to prune cache_W. Search ranges\n",
    "# can be made equal for each unique weight in a sequential manner)\n",
    "# Use multiple caches which are dependant / independent\n",
    "# May do distance calculation between new weight-activation pairs and cache pairs in batches to increase the batch size fed to\n",
    "# the cache\n",
    "\n",
    "# First: Make updating time corresponding to hits consume less memory and run faster. (Start sampling LI from the end in\n",
    "# small batches and break from the loop when all unique_LI have been found)\n",
    "# Currently, it is the bottleneck for increasing batch size. So even if say 16 caches are used in parallel, only the same no \n",
    "# of hits as are being processed currently could be processed in parallel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import  torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from statistics import mean\n",
    "from collections  import OrderedDict\n",
    "from collections  import namedtuple\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../datasets/ILSVRC2012_img_val - Retrain/'\n",
    "dataset = {x:datasets.ImageFolder(os.path.join(data_dir, x), transform[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {x:torch.utils.data.DataLoader(dataset[x], batch_size = 1, shuffle = False)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {x:len(dataset[x]) for x in ['train', 'val']}\n",
    "class_names = dataset['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cache:\n",
    "    global device\n",
    "    \n",
    "    def __init__(self, num, size, epsilon):\n",
    "        self.size = size\n",
    "        self.num = num\n",
    "        self.W = torch.randn((self.num, self.size), device = device)\n",
    "        self.A = torch.randn((self.num, self.size), device = device)\n",
    "        self.time = torch.ones((self.num, self.size), device = device, dtype = torch.int32)\n",
    "        self.epsilon = epsilon\n",
    "        self.hits = torch.zeros((self.num,), device = device, dtype = torch.int64)\n",
    "        self.misses = torch.zeros((self.num,), device = device, dtype = torch.int64)\n",
    "        \n",
    "    def batch_wise_approx(self, orig_W, orig_A):\n",
    "        \n",
    "        s1 = orig_W.shape\n",
    "        s2 = orig_A.shape\n",
    "\n",
    "        orig_W = orig_W.flatten()\n",
    "        orig_A = orig_A.flatten()\n",
    "        if orig_W.shape != orig_A.shape:\n",
    "            sys.exit(\"W-shape and A-shape unequal\")\n",
    "\n",
    "        # Remove indices where W = 0 or A = 0\n",
    "        \n",
    "        non_zero_mask = ((orig_W != 0) & (orig_A != 0))\n",
    "        W = orig_W[non_zero_mask]\n",
    "        A = orig_A[non_zero_mask]\n",
    "        \n",
    "        # Split into batches\n",
    "        \n",
    "        out_W = torch.zeros(W.shape, dtype = torch.float32, device = device)\n",
    "        out_A = torch.zeros(A.shape, dtype = torch.float32, device = device)\n",
    "        num_elem = W.shape[0]\n",
    "        period = 500\n",
    "        \n",
    "        upper_lim = int(int(num_elem / self.num) / period)\n",
    "\n",
    "        for i in range(int(num_elem / self.num) + 1):\n",
    "\n",
    "            start = i * self.num\n",
    "            end = min((i + 1) * self.num, num_elem)\n",
    "            if end == start:\n",
    "                break\n",
    "            \n",
    "            out_W[start:end], out_A[start:end] = self.approx(W[start:end], A[start:end])\n",
    "            \n",
    "            if i % period == 0:\n",
    "                \n",
    "                hits = torch.sum(self.hits)\n",
    "                misses = torch.sum(self.misses)\n",
    "                tot = hits + misses\n",
    "                \n",
    "                self.hits = torch.zeros((self.num,), device = device, dtype = torch.int64)\n",
    "                self.misses = torch.zeros((self.num,), device = device, dtype = torch.int64)\n",
    "                \n",
    "                print('\\r{} / {} | Hits {} / {}, {:.2f}%'.format(int(i / period), upper_lim, hits, tot,  hits * 100.0 / tot), end = '', flush = True)\n",
    "                \n",
    "\n",
    "        orig_W[non_zero_mask] = out_W\n",
    "        orig_A[non_zero_mask] = out_A\n",
    "    \n",
    "        return orig_W.view(s1), orig_A.view(s2)\n",
    "          \n",
    "    def approx(self, W, A):\n",
    "                \n",
    "        N = W.shape[0]\n",
    "        \n",
    "        if (N > self.num):\n",
    "            sys.exit(\"Invalid number of W / A provided\")\n",
    "        \n",
    "        # W and A are flat\n",
    "        # They have no zero elements\n",
    "        \n",
    "        if (W.shape != (N,)) or (A.shape != (N,)):\n",
    "            sys.exit(\"W and A have incorrect shape\")\n",
    "            \n",
    "        my_W = self.W[:N]\n",
    "        my_A = self.A[:N]\n",
    "        my_time = self.time[:N]\n",
    "        my_hits = self.hits[:N]\n",
    "        my_misses = self.misses[:N]\n",
    "        \n",
    "        # Find nearest cache elements and their distances\n",
    "        \n",
    "        I = torch.cat([W.view(-1, 1), A.view(-1, 1)], dim = 1).unsqueeze(2)\n",
    "        S = torch.cat([my_W.unsqueeze(1), my_A.unsqueeze(1)], dim = 1)\n",
    "        dist, LI = torch.abs(I - S).sum(dim = 1).min(dim = 1)\n",
    "        \n",
    "        # Find hits, misses\n",
    "        \n",
    "        hits = (dist < self.epsilon)\n",
    "        misses = ~hits\n",
    "        num_hits = torch.sum(hits).item()\n",
    "        num_misses = N - num_hits\n",
    "        \n",
    "        # Hit Processing\n",
    "        \n",
    "        if num_hits > 0:\n",
    "            my_hits[hits] += 1\n",
    "            \n",
    "            # Update time\n",
    "            my_time[hits] += 1\n",
    "            rows = torch.arange(0, N, device = device)[hits].long()\n",
    "            my_time[rows, LI[hits]] = 0\n",
    "            \n",
    "            # Update W, A\n",
    "            W[hits] = my_W[rows, LI[hits]]\n",
    "            A[hits] = my_A[rows, LI[hits]]\n",
    "            \n",
    "        # Miss Processing\n",
    "        \n",
    "        if num_misses > 0:\n",
    "            my_misses[misses] += 1\n",
    "            \n",
    "            # Update cache_store\n",
    "            rows = torch.arange(0, N, device = device)[misses].long()\n",
    "            _, rem_inds = torch.max(my_time[misses], dim = 1)\n",
    "            my_W[rows, rem_inds] = W[misses]\n",
    "            my_A[rows, rem_inds] = A[misses]\n",
    "            \n",
    "            # Update time\n",
    "            my_time[misses] += 1\n",
    "            my_time[rows, rem_inds] = 0\n",
    "            \n",
    "        return W, A\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_cache = cache(16, 200000, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing\n",
    "\n",
    "# W = torch.tensor([1,2,9,3], dtype = torch.float32).flatten()\n",
    "# A = torch.tensor([2,6,0,1], dtype = torch.float32).flatten()\n",
    "# cache = cache(4, 2, 4)\n",
    "# cache.W[0] = torch.tensor([4, 10])\n",
    "# cache.A[0] = torch.tensor([2, 5])\n",
    "# cache.W[1] = torch.tensor([4, 10])\n",
    "# cache.A[1] = torch.tensor([2, 5])\n",
    "# cache.W[2] = torch.tensor([4, 10])\n",
    "# cache.A[2] = torch.tensor([2, 5])\n",
    "# cache.W[3] = torch.tensor([4, 10])\n",
    "# cache.A[3] = torch.tensor([2, 5])\n",
    "\n",
    "# W_, A_ = cache.approx(W, A)\n",
    "# # W_ tensor([4., 2., 9., 4.])\n",
    "# # A_ tensor([2., 6., 0., 2.])\n",
    "# # cache.W tensor([[ 4., 10.],\n",
    "# #                 [ 4.,  2.],\n",
    "# #                 [ 4.,  9.],\n",
    "# #                 [ 4., 10.]])\n",
    "# # cache.A tensor([[2., 5.],\n",
    "# #                 [2., 6.],\n",
    "# #                 [2., 0.],\n",
    "# #                 [2., 5.]])\n",
    "# # cache.time tensor([[0, 2],\n",
    "# #                   [2, 0],\n",
    "# #                   [2, 0],\n",
    "# #                   [0, 2]], dtype=torch.int32)\n",
    "# # cache.hits tensor([1, 0, 0, 1])\n",
    "# # cache.misses tensor([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cached_conv(nn.Module):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    def __init__(self, wt_layer, cache):\n",
    "        super(cached_conv, self).__init__()\n",
    "        self.weight = wt_layer.weight\n",
    "        self.bias = wt_layer.bias\n",
    "        self.stride = wt_layer.stride\n",
    "        self.padding = wt_layer.padding\n",
    "        #self.dilation = wt_layer.dilation\n",
    "        #self.groups = wt_layer.groups\n",
    "        self.cache = cache\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print('\\nReached conv')\n",
    "        \n",
    "        A_prev = x\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        stride = self.stride\n",
    "        pad = self.padding\n",
    "        cache = self.cache\n",
    "        \n",
    "        #return F.conv2d(x, W, bias = b, stride = stride, padding = pad)\n",
    "        \n",
    "        (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
    "        (n_C, n_C_prev, f, f) = W.shape\n",
    "        \n",
    "        # Compute the dimensions of the CONV output volume \n",
    "        n_H = int((n_H_prev + 2*pad[0] - f)/stride[0]) + 1\n",
    "        n_W =int((n_W_prev + 2*pad[1] - f)/stride[1]) + 1\n",
    "\n",
    "        y = F.unfold(A_prev, (f, f), padding = pad, stride = stride).transpose(2,1)\n",
    "        #y = y.view(m, 1, y.shape[1],y.shape[2]).repeat((1,n_C,1,1))\n",
    "        y = y.view(m, 1, y.shape[1],y.shape[2]).expand((-1,n_C,-1,-1))\n",
    "\n",
    "        W = W.view(n_C, -1)\n",
    "        #W = W.view(1,n_C, 1, W.shape[1]).repeat(m, 1, y.shape[2], 1)\n",
    "        W = W.view(1,n_C, 1, W.shape[1]).expand(m, -1, y.shape[2], -1)\n",
    "        \n",
    "        W, y = cache.batch_wise_approx(W, y)\n",
    "        \n",
    "        Z = torch.sum(W * y, dim = 3).view(m, n_C, n_H, n_W)\n",
    "        Z = Z + b.view(1,b.shape[0], 1, 1)\n",
    "        \n",
    "        #print(torch.sum(torch.abs(Z - F.conv2d(x, self.weight, bias = self.bias, stride = self.stride, padding = self.padding))))\n",
    "        \n",
    "        #sys.exit()\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cached_fc(nn.Module):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    def __init__(self, wt_layer, cache):\n",
    "        super(cached_fc, self).__init__()\n",
    "        self.weight = wt_layer.weight\n",
    "        self.bias = wt_layer.bias\n",
    "        self.cache = cache\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print('\\nReached fc')\n",
    "               \n",
    "        #return F.linear(x, self.weight, bias = self.bias)\n",
    "        \n",
    "        A_prev = x\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        cache = self.cache\n",
    "        \n",
    "        (m, n_prev) = A_prev.shape\n",
    "        (n, n_prev) = W.shape\n",
    "\n",
    "        A_prev = A_prev.view(m, 1, n_prev).expand(-1, n, -1)\n",
    "        W = W.view(1, n, n_prev).expand(m, -1, -1)\n",
    "\n",
    "        W, A_prev = cache.batch_wise_approx(W, A_prev)\n",
    "        \n",
    "        Z = (A_prev * W).sum(dim = 2).view(m, n)\n",
    "        Z = Z + b.view(1, n)\n",
    "        \n",
    "        #print(torch.sum(torch.abs(Z - F.linear(x, self.weight, bias = self.bias))))\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, init_state_dict, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        self.load_state_dict(init_state_dict)\n",
    "        \n",
    "        self.init_cache_layers()\n",
    "       \n",
    "    def init_cache_layers(self):\n",
    "        \n",
    "        ind = -1\n",
    "        global global_cache \n",
    "        q_list = []\n",
    "        for layer in self.features:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                ind += 1\n",
    "                q_list.append(cached_conv(layer, global_cache))\n",
    "            else:\n",
    "                q_list.append(layer)\n",
    "        self.features = nn.Sequential(*q_list)\n",
    "        \n",
    "        ind = -1\n",
    "        q_list = []\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                ind += 1\n",
    "                q_list.append(cached_fc(layer, global_cache))\n",
    "            else:\n",
    "                q_list.append(layer)\n",
    "        self.classifier = nn.Sequential(*q_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, phase):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "#     if record_grad:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "\n",
    "        \n",
    "    done = 0\n",
    "    acc = 0.0\n",
    "    since = time.time()\n",
    "    corrects = torch.tensor(0)\n",
    "    total_loss = 0.0\n",
    "    corrects = corrects.to(device)\n",
    "    loss = 100.0\n",
    "    \n",
    "    for inputs, labels in dataloader[phase]:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            corrects += torch.sum(preds == labels)\n",
    "\n",
    "        done += len(inputs)\n",
    "        print('\\r{}, {}, {:.2f}%, {:.2f}'.format(corrects.item(), done, corrects.item() * 100.0 / done, total_loss), end = '')\n",
    "#         if done >= 1000:\n",
    "#             break\n",
    "                    \n",
    "    acc = corrects.double() / done\n",
    "    print('\\n{} Acc: {:.4f} %'.format(phase, acc * 100))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Total time taken = {} seconds'.format(time_elapsed))\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "model = AlexNet(init_state_dict=alexnet.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "check_accuracy(model, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
