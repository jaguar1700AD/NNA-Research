{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements weight sharing\n",
    "# In wt layer initialization, kmeans should not be applied on zero valued weights\n",
    "# Zero valued weights should also not be part of any replacement during forward pass\n",
    "# Store the mask of non zero weights and do weight_layer[mask] = quant_vals[indices]\n",
    "\n",
    "# Print the % of zero weights continuously for error checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import  torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from statistics import mean\n",
    "from collections  import OrderedDict\n",
    "import sys\n",
    "\n",
    "device = torch.device('cuda')\n",
    "SAVE_PATH = 'D://models//wt_shared_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:\\\\datasets\\\\ILSVRC2012_img_val - Retrain\\\\'\n",
    "dataset = {x:datasets.ImageFolder(os.path.join(data_dir, x), transform[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {x:torch.utils.data.DataLoader(dataset[x], batch_size = 64, shuffle = False, num_workers = 6, pin_memory=True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_actk = 24\n",
    "dataloader_actk = torch.utils.data.DataLoader(dataset['train'], batch_size = batch_size_actk, shuffle = True, num_workers = 6, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {x:len(dataset[x]) for x in ['train', 'val']}\n",
    "class_names = dataset['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantize(nn.Module):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    def __init__(self, layer_type, wt_layer, num_vals, quick = False):\n",
    "        super(Quantize, self).__init__()\n",
    "        \n",
    "        if layer_type != 'conv' and layer_type != 'fc':\n",
    "            sys.exit(\"Invalid layer type given\")\n",
    "        \n",
    "        if quick == False:\n",
    "            flat_mat = wt_layer.weight.to('cpu').view(-1, 1).detach()\n",
    "            #kmeans = KMeans(n_clusters=quant_nums[i], n_jobs=12)\n",
    "            kmeans = MiniBatchKMeans(n_clusters=num_vals, batch_size=1000000)\n",
    "            kmeans.fit(flat_mat)\n",
    "            self.centroids = nn.Parameter(torch.from_numpy(kmeans.cluster_centers_).requires_grad_(True))\n",
    "            self.labels = nn.Parameter(torch.from_numpy(kmeans.labels_).view(wt_layer.weight.shape).type(torch.long), requires_grad=False)\n",
    "        else:\n",
    "            self.centroids = nn.Parameter(torch.zeros((num_vals,1)).requires_grad_(True))\n",
    "            self.labels = nn.Parameter(torch.zeros(wt_layer.weight.shape, dtype = torch.long), requires_grad=False)\n",
    "        self.type = layer_type    \n",
    "        self.num_reps = num_vals\n",
    "        self.bias = wt_layer.bias.requires_grad_(True)\n",
    "        if layer_type == 'conv':\n",
    "            self.stride = wt_layer.stride\n",
    "            self.padding = wt_layer.padding\n",
    "            self.dilation = wt_layer.dilation\n",
    "            self.groups = wt_layer.groups\n",
    "        \n",
    "    def forward(self, x):\n",
    "        wt = torch.squeeze(self.centroids[self.labels], dim = -1).type(torch.float32)\n",
    "        if self.type == 'conv':\n",
    "            return F.conv2d(x, wt, bias = self.bias, stride = self.stride, padding = self.padding, dilation = self.dilation, groups = self.groups)\n",
    "        else:\n",
    "            return F.linear(x, wt, bias = self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, init_model, quant_nums_w, quick = False, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        if init_model != None:\n",
    "            self.load_state_dict(copy.deepcopy(init_model.state_dict()))\n",
    "        \n",
    "        if quant_nums_w != None:\n",
    "            self.init_wt_quantizers(quant_nums_w, quick = quick)\n",
    "       \n",
    "    def init_wt_quantizers(self, quant_nums, quick):\n",
    "        \n",
    "        ind = -1\n",
    "        \n",
    "        q_list = []\n",
    "        for layer in self.features:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                ind += 1\n",
    "                q_list.append(Quantize('conv', layer, quant_nums[ind], quick))\n",
    "                print('Done', ind)\n",
    "            else:\n",
    "                q_list.append(layer)\n",
    "        self.features = nn.Sequential(*q_list)\n",
    "        \n",
    "        q_list = []\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                ind += 1\n",
    "                q_list.append(Quantize('fc', layer, quant_nums[ind], quick))\n",
    "                print('Done', ind)\n",
    "            else:\n",
    "                q_list.append(layer)\n",
    "        self.classifier = nn.Sequential(*q_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, phase, record_grad, criterion = None, optimizer = None, I = None):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "#     if record_grad:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "\n",
    "        \n",
    "    done = 0\n",
    "    acc = 0.0\n",
    "    since = time.time()\n",
    "    corrects = torch.tensor(0)\n",
    "    total_loss = 0.0\n",
    "    corrects = corrects.to(device)\n",
    "    loss = 100.0\n",
    "    \n",
    "    if I == None:\n",
    "        for inputs, labels in dataloader[phase]:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if record_grad:\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    corrects += torch.sum(preds == labels)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            else:\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    corrects += torch.sum(preds == labels)\n",
    "                    \n",
    "            done += len(inputs)\n",
    "            print('\\r{}, {}, {:.2f}%, {:.2f}'.format(corrects.item(), done, corrects.item() * 100.0 / done, total_loss), end = '')\n",
    "#             if done >= 64:\n",
    "#                 break\n",
    "                    \n",
    "    else:\n",
    "            \n",
    "        inputs, labels = I\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if record_grad:\n",
    "            with torch.set_grad_enabled(True):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects += torch.sum(preds == labels)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #print(model.features[0].centroids.grad)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        else:\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects += torch.sum(preds == labels)\n",
    "                \n",
    "        done += len(inputs)\n",
    "        print('\\r{}, {}, {:.2f}%, {:.2f}'.format(corrects.item(), done, corrects.item() * 100.0 / done, total_loss), end = '')\n",
    "\n",
    "    acc = corrects.double() / done\n",
    "    print('\\n{} Acc: {:.4f} %'.format(phase, acc * 100))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Total time taken = {} seconds'.format(time_elapsed))\n",
    "\n",
    "    if record_grad:\n",
    "        return acc, total_loss\n",
    "    else:\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, criterion, optimizer, scheduler = None, num_epochs = 25, I = None):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    print('          ', end = '\\r')\n",
    "    acc = {'train':0.0, 'val':0.0}\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    since = time.time()\n",
    "    torch.save(model.state_dict(), SAVE_PATH)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        phase = 'train'\n",
    "        epoch_acc, epoch_loss = check_accuracy(model, phase, criterion=criterion, optimizer=optimizer, record_grad = True, I = I)\n",
    "        epoch_acc = epoch_acc / dataset_size[phase]\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "                \n",
    "        print()\n",
    "        \n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(torch.load(SAVE_PATH))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.load_state_dict(torch.load('D://models//undone_pruned_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = alexnet\n",
    "# model.to(device)\n",
    "# check_accuracy(model, 'val', record_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 0\n",
      "Done 1\n",
      "Done 2\n",
      "Done 3\n",
      "Done 4\n",
      "Done 5\n",
      "Done 6\n",
      "Done 7\n"
     ]
    }
   ],
   "source": [
    "ORIG_PATH = 'D://models//NNA_quants.pth'\n",
    "\n",
    "model = AlexNet(init_model=alexnet, quant_nums_w = [32]*8, quick = False)\n",
    "torch.save(model.state_dict(), ORIG_PATH)\n",
    "\n",
    "#model = AlexNet(init_model=alexnet, quant_nums_w = [32]*8, quick = True)\n",
    "#model.load_state_dict(torch.load(ORIG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "18883, 40000, 47.21%, 98149.64\n",
      "train Acc: 47.2075 %\n",
      "Total time taken = 1877.7175433635712 seconds\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "18865, 40000, 47.16%, 97592.98\n",
      "train Acc: 47.1625 %\n",
      "Total time taken = 1880.397887468338 seconds\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "18961, 40000, 47.40%, 97289.71\n",
      "train Acc: 47.4025 %\n",
      "Total time taken = 1880.6317908763885 seconds\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "18819, 40000, 47.05%, 97536.71\n",
      "train Acc: 47.0475 %\n",
      "Total time taken = 1880.1079800128937 seconds\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "18953, 40000, 47.38%, 96942.33\n",
      "train Acc: 47.3825 %\n",
      "Total time taken = 1879.839186668396 seconds\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "18972, 40000, 47.43%, 96953.85\n",
      "train Acc: 47.4300 %\n",
      "Total time taken = 1880.4326536655426 seconds\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "19006, 40000, 47.52%, 97370.75\n",
      "train Acc: 47.5150 %\n",
      "Total time taken = 1881.1595783233643 seconds\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "19042, 40000, 47.60%, 96701.83\n",
      "train Acc: 47.6050 %\n",
      "Total time taken = 1879.9208130836487 seconds\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "18975, 40000, 47.44%, 96896.90\n",
      "train Acc: 47.4375 %\n",
      "Total time taken = 1880.1195380687714 seconds\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "19011, 40000, 47.53%, 96658.51\n",
      "train Acc: 47.5275 %\n",
      "Total time taken = 1883.9600541591644 seconds\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "18867, 40000, 47.17%, 96913.61\n",
      "train Acc: 47.1675 %\n",
      "Total time taken = 1879.1100873947144 seconds\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "18993, 40000, 47.48%, 96975.68\n",
      "train Acc: 47.4825 %\n",
      "Total time taken = 1879.3047659397125 seconds\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "18933, 40000, 47.33%, 97163.79\n",
      "train Acc: 47.3325 %\n",
      "Total time taken = 1878.7399139404297 seconds\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "18941, 40000, 47.35%, 97104.80\n",
      "train Acc: 47.3525 %\n",
      "Total time taken = 1878.6612405776978 seconds\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "18984, 40000, 47.46%, 96584.16\n",
      "train Acc: 47.4600 %\n",
      "Total time taken = 1879.4979784488678 seconds\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "18903, 40000, 47.26%, 96952.37\n",
      "train Acc: 47.2575 %\n",
      "Total time taken = 1878.825118303299 seconds\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "19103, 40000, 47.76%, 96457.26\n",
      "train Acc: 47.7575 %\n",
      "Total time taken = 1880.0410249233246 seconds\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "19035, 40000, 47.59%, 96484.05\n",
      "train Acc: 47.5875 %\n",
      "Total time taken = 1877.4364631175995 seconds\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "6257, 11776, 53.13%, 23787.41"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b57b26299440>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-850d66d48fa9>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs, I)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mphase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mepoch_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mepoch_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_acc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch_acc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-2147d88133d7>\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[1;34m(model, phase, record_grad, criterion, optimizer, I)\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[0mcorrects\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.SGD(params, lr = 1e-7, momentum = 0.9)\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.5)\n",
    "exp_lr_scheduler = None\n",
    "#I = next(iter(dataloader['train']))\n",
    "I = None\n",
    "\n",
    "model = main(model, criterion, optimizer, exp_lr_scheduler, num_epochs = 100, I = I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
