{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements 1 level cache (No backward pass needed so not implemented; only accuracy determination required)\n",
    "# Weight sharing not included\n",
    "\n",
    "# Cache.approx just updates W, A values according to hits and misses and does not compute the result. Actual result computation\n",
    "# is done at the end in parallel by doing updated_W * updated_A\n",
    "# Current - LRU policy replacement. Future - May use LRU + count-used based policy\n",
    "\n",
    "# See why accuracy is lower than normal. (17% vs 65%)\n",
    "# May use expand instead of repeat to speed up computation\n",
    "\n",
    "\n",
    "# Find out how hdw simulation is done in papers. They ofcourse don't actually build the hdw, they just simulate it. But, \n",
    "# they are still able to find the accuracy value by running the program over their hdw simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import  torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from statistics import mean\n",
    "from collections  import OrderedDict\n",
    "from collections  import namedtuple\n",
    "import sys\n",
    "\n",
    "device = torch.device('cuda')\n",
    "SAVE_PATH = 'D://models//NNA_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:\\\\datasets\\\\ILSVRC2012_img_val - Retrain\\\\'\n",
    "dataset = {x:datasets.ImageFolder(os.path.join(data_dir, x), transform[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {x:torch.utils.data.DataLoader(dataset[x], batch_size = 1, shuffle = False)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {x:len(dataset[x]) for x in ['train', 'val']}\n",
    "class_names = dataset['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cache:\n",
    "    global device\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.size = 5000\n",
    "        self.W = torch.randn(self.size, device = device)\n",
    "        self.A = torch.randn(self.size, device = device)\n",
    "        self.time = torch.ones(self.size, device = device, dtype = torch.int32)\n",
    "        self.epsilon = 1e-2\n",
    "        self.hits = torch.tensor(0, device = device, dtype = torch.float32).view(1)\n",
    "        self.misses = torch.tensor(0, device = device, dtype = torch.float32).view(1)\n",
    "        \n",
    "    def batch_wise_approx(self, orig_W, orig_A, num_send):\n",
    "        \n",
    "        if orig_W.shape != orig_A.shape:\n",
    "            sys.exit(\"W-shape and A-shape unequal\")\n",
    "        \n",
    "        shape = orig_W.shape\n",
    "        num_elem = orig_W.numel()\n",
    "        \n",
    "        orig_W = orig_W.flatten()\n",
    "        orig_A = orig_A.flatten()\n",
    "        \n",
    "        out_W = torch.zeros(orig_W.shape, dtype = torch.float32, device = device)\n",
    "        out_A = torch.zeros(orig_A.shape, dtype = torch.float32, device = device)\n",
    "        \n",
    "        for i in range(int(num_elem / num_send) + 1):\n",
    "            start = i * num_send\n",
    "            end = min((i + 1) * num_send, num_elem)\n",
    "            out_W[start:end], out_A[start:end] = self.approx(orig_W[start:end], orig_A[start:end])\n",
    "            \n",
    "        return out_W.view(shape), out_A.view(shape)\n",
    "        \n",
    "    def approx(self, orig_W, orig_A):\n",
    "        \n",
    "        s1 = orig_W.shape\n",
    "        s2 = orig_A.shape\n",
    "        \n",
    "        orig_W = orig_W.flatten()\n",
    "        orig_A = orig_A.flatten()\n",
    "        if orig_W.shape != orig_A.shape:\n",
    "            sys.exit(\"W-shape and A-shape unequal\")\n",
    "        \n",
    "        # Remove indices where W = 0 or A = 0\n",
    "        \n",
    "        ind1 = (orig_W == 0).nonzero() \n",
    "        ind2 = (orig_A == 0).nonzero()\n",
    "        zero_inds = torch.unique(torch.cat((ind1, ind2))).view(-1,1)\n",
    "        all_inds = torch.arange(orig_W.shape[0], device = device).view(1,-1)\n",
    "        non_zero_inds = (((zero_inds - all_inds) == 0).sum(dim = 0) == 0).nonzero().view(-1) # set(all_inds) - set(zero_inds)\n",
    "        W_ = orig_W[non_zero_inds]\n",
    "        A_ = orig_A[non_zero_inds]\n",
    "        \n",
    "        W = W_\n",
    "        A = A_\n",
    "        \n",
    "        while(True):\n",
    "            \n",
    "            # Find hits, misses\n",
    "\n",
    "            I = torch.cat((W.view(1,-1),A.view(1,-1)), dim = 0)\n",
    "            S = torch.cat((self.W.view(1,-1), self.A.view(1,-1)), dim = 0)\n",
    "\n",
    "            x = I.repeat(S.shape[1],1,1)\n",
    "            y = S.t().view(S.shape[1],2,1).repeat(1,1,I.shape[1])\n",
    "            dist, LI = torch.abs(x - y).sum(dim = 1).min(dim = 0)\n",
    "\n",
    "            misses = ~(dist < self.epsilon)\n",
    "            t = misses.nonzero()\n",
    "            \n",
    "            if t.shape[0] != 0:\n",
    "                max_lim = t[0]\n",
    "            else:\n",
    "                max_lim = W.shape[0]\n",
    "            LI = LI[:max_lim]\n",
    "            \n",
    "            #print('LI', LI)\n",
    "            \n",
    "            if LI.shape[0] != 0: # Hits encountered\n",
    "                \n",
    "                # Update num hits\n",
    "                self.hits += max_lim\n",
    "\n",
    "                # Update W, A corresponding to hits\n",
    "                W[:max_lim] = self.W[LI]\n",
    "                A[:max_lim] = self.A[LI]\n",
    "\n",
    "                # Update time corresponding to hits\n",
    "\n",
    "                used = torch.unique(LI)\n",
    "                x = LI.view(-1,1) == used.view(1,-1)\n",
    "                x = x.type(torch.int32)\n",
    "                x = x * torch.arange(start = 1, end = x.shape[0]+1, device = device).view(-1,1).expand_as(x)\n",
    "                x = x.shape[0] - 1 - torch.argmax(x, dim = 0)\n",
    "\n",
    "                self.time += LI.shape[0]\n",
    "                self.time[used] = 0\n",
    "                self.time[used] += x\n",
    "\n",
    "#                 print('Time', self.time)\n",
    "#                 print('W', W)\n",
    "#                 print('A', A)\n",
    "            \n",
    "            if t.shape[0] != 0: # A miss encountered\n",
    "                \n",
    "                self.misses += 1\n",
    "                \n",
    "                # Update stored values in cache\n",
    "                ind = torch.argmax(self.time)\n",
    "                self.W[ind] = W[max_lim]\n",
    "                self.A[ind] = A[max_lim]\n",
    "                self.time += 1\n",
    "                self.time[ind] = 0\n",
    "                \n",
    "#                 print('Miss at', max_lim, W[max_lim], A[max_lim])\n",
    "#                 print('New cache W', self.W)\n",
    "#                 print('New cache A', self.A)\n",
    "#                 print('New time', self.time)\n",
    "#                 print()\n",
    "                \n",
    "                if max_lim == W.shape[0] - 1:\n",
    "                    break\n",
    "                \n",
    "                # Update W, A, prod for next cycle\n",
    "                W = W[max_lim + 1:]\n",
    "                A = A[max_lim + 1:]\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            hits = self.hits.item()\n",
    "            misses = self.misses.item()\n",
    "            tot = hits + misses\n",
    "            print('\\r{} / {}, {:.2f}%'.format(hits, tot,  hits * 100.0 / tot), end = '')\n",
    "            if tot > 1000:\n",
    "                self.hits = torch.tensor(0, device = device, dtype = torch.float32).view(1)\n",
    "                self.misses = torch.tensor(0, device = device, dtype = torch.float32).view(1)     \n",
    "        \n",
    "        orig_W[non_zero_inds] = W_\n",
    "        orig_A[non_zero_inds] = A_\n",
    "        \n",
    "        return orig_W.view(s1), orig_A.view(s2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_cache = cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# global_cache.size = 3\n",
    "# global_cache.epsilon = 3\n",
    "# global_cache.W = torch.tensor([1.,2,3], device = device, dtype = torch.float32)\n",
    "# global_cache.A = torch.tensor([7.,2,5], device = device, dtype = torch.float32)\n",
    "# global_cache.time = torch.ones(global_cache.size, device = device, dtype = torch.int32)\n",
    "\n",
    "# x = torch.tensor([1., 2, 3, 1, 2, 1, 1, 3, 0, 3, 2], device = device, dtype = torch.float32)\n",
    "# y = torch.tensor([1., 2, 3, 4, 5, 6, 7, 8, 9, 10, 0], device = device, dtype = torch.float32)\n",
    "# x, y = global_cache.approx(x, y)\n",
    "\n",
    "# print()\n",
    "# print(global_cache.hits, global_cache.misses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([])\n",
    "torch.cat((x, torch.zeros(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cached_conv(nn.Module):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    def __init__(self, wt_layer, cache):\n",
    "        super(cached_conv, self).__init__()\n",
    "        self.weight = wt_layer.weight\n",
    "        self.bias = wt_layer.bias\n",
    "        self.stride = wt_layer.stride\n",
    "        self.padding = wt_layer.padding\n",
    "        #self.dilation = wt_layer.dilation\n",
    "        #self.groups = wt_layer.groups\n",
    "        self.cache = cache\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        A_prev = x\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        stride = self.stride\n",
    "        pad = self.padding\n",
    "        cache = self.cache\n",
    "        \n",
    "        #return F.conv2d(x, W, bias = b, stride = stride, padding = pad)\n",
    "        \n",
    "        (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
    "        (n_C, n_C_prev, f, f) = W.shape\n",
    "        \n",
    "        # Compute the dimensions of the CONV output volume \n",
    "        n_H = int((n_H_prev + 2*pad[0] - f)/stride[0]) + 1\n",
    "        n_W =int((n_W_prev + 2*pad[1] - f)/stride[1]) + 1\n",
    "\n",
    "        y = F.unfold(A_prev, (f, f), padding = pad, stride = stride).transpose(2,1)\n",
    "        #y = y.view(m, 1, y.shape[1],y.shape[2]).repeat((1,n_C,1,1))\n",
    "        y = y.view(m, 1, y.shape[1],y.shape[2]).expand((-1,n_C,-1,-1))\n",
    "\n",
    "        W = W.view(n_C, -1)\n",
    "        #W = W.view(1,n_C, 1, W.shape[1]).repeat(m, 1, y.shape[2], 1)\n",
    "        W = W.view(1,n_C, 1, W.shape[1]).expand(m, -1, y.shape[2], -1)\n",
    "        \n",
    "        W, y = cache.batch_wise_approx(W, y, 10000)\n",
    "        \n",
    "        Z = torch.sum(W * y, dim = 3).view(m, n_C, n_H, n_W)\n",
    "        Z = Z + b.view(1,b.shape[0], 1, 1)\n",
    "        \n",
    "        #print(torch.sum(torch.abs(Z - F.conv2d(x, self.weight, bias = self.bias, stride = self.stride, padding = self.padding))))\n",
    "        \n",
    "        #sys.exit()\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, init_model, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        self.load_state_dict(copy.deepcopy(init_model.state_dict()))\n",
    "        \n",
    "        self.init_cache_layers()\n",
    "       \n",
    "    def init_cache_layers(self):\n",
    "        \n",
    "        ind = -1\n",
    "        global global_cache \n",
    "        q_list = []\n",
    "        for layer in self.features:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                ind += 1\n",
    "                q_list.append(cached_conv(layer, global_cache))\n",
    "            else:\n",
    "                q_list.append(layer)\n",
    "        self.features = nn.Sequential(*q_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, phase):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "#     if record_grad:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "\n",
    "        \n",
    "    done = 0\n",
    "    acc = 0.0\n",
    "    since = time.time()\n",
    "    corrects = torch.tensor(0)\n",
    "    total_loss = 0.0\n",
    "    corrects = corrects.to(device)\n",
    "    loss = 100.0\n",
    "    \n",
    "    for inputs, labels in dataloader[phase]:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            corrects += torch.sum(preds == labels)\n",
    "\n",
    "        done += len(inputs)\n",
    "        print('\\r{}, {}, {:.2f}%, {:.2f}'.format(corrects.item(), done, corrects.item() * 100.0 / done, total_loss), end = '')\n",
    "        if done >= 1000:\n",
    "            break\n",
    "                    \n",
    "    acc = corrects.double() / done\n",
    "    print('\\n{} Acc: {:.4f} %'.format(phase, acc * 100))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Total time taken = {} seconds'.format(time_elapsed))\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PATH = 'D://models//NNA_quants.pth'\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "model = AlexNet(init_model=alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): cached_conv()\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): cached_conv()\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): cached_conv()\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): cached_conv()\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): cached_conv()\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225.0 / 246.0, 91.46%%%7%%"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "check_accuracy(model, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
