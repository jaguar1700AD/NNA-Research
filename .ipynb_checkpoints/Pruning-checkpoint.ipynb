{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Left - Remove the pruning layers as if they never existed and save the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import  torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from statistics import mean\n",
    "from collections  import OrderedDict\n",
    "from collections  import namedtuple\n",
    "import sys\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "device = torch.device('cuda')\n",
    "SAVE_PATH = 'D://models//Pruned_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:\\\\datasets\\\\ILSVRC2012_img_val - Retrain\\\\'\n",
    "dataset = {x:datasets.ImageFolder(os.path.join(data_dir, x), transform[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {x:torch.utils.data.DataLoader(dataset[x], batch_size = 256, shuffle = False, num_workers = 6, pin_memory = True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {x:len(dataset[x]) for x in ['train', 'val']}\n",
    "class_names = dataset['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, init_model, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        self.load_state_dict(copy.deepcopy(init_model.state_dict()))\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, phase, record_grad, criterion = None, optimizer = None):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "#     if record_grad:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "\n",
    "        \n",
    "    done = 0\n",
    "    acc = 0.0\n",
    "    since = time.time()\n",
    "    corrects = torch.tensor(0)\n",
    "    total_loss = 0.0\n",
    "    corrects = corrects.to(device)\n",
    "    loss = 100.0\n",
    "    \n",
    "    for inputs, labels in dataloader[phase]:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if record_grad:\n",
    "            with torch.set_grad_enabled(True):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects += torch.sum(preds == labels)\n",
    "                loss = criterion(outputs, labels)\n",
    "                print(model.features[10].weight.grad)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            done += len(inputs)\n",
    "            print('\\r{}, {}, {:.2f}%, {:.2f}, {:.2f}'.format(corrects.item(), done, corrects.item() * 100.0 / done, loss.item(), total_loss), end = '')\n",
    "\n",
    "        else:\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects += torch.sum(preds == labels)\n",
    "\n",
    "            done += len(inputs)\n",
    "            print('\\r{}, {}, {:.2f}%'.format(corrects.item(), done, corrects.item() * 100.0 / done), end = '')\n",
    "\n",
    "    acc = corrects.double() / done\n",
    "    print('\\n{} Acc: {:.4f} %'.format(phase, acc * 100))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Total time taken = {} seconds'.format(time_elapsed))\n",
    "\n",
    "    if record_grad:\n",
    "        return acc, total_loss\n",
    "    else:\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_limited(model, criterion, optimizer, num_epochs = 100, do_baseline = True):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    print('          ', end = '\\r')\n",
    "    acc = {'train':0.0, 'val':0.0}\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    if do_baseline:\n",
    "        acc['train'] = check_accuracy(model, phase = 'train', record_grad = False)\n",
    "        print('.......... Baseline Evaluation Done ..............')\n",
    "        best_acc = acc['train']\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        epoch_acc, epoch_loss = check_accuracy(model, phase='train', record_grad=True, criterion=criterion, optimizer=optimizer)\n",
    "        if epoch_acc > best_acc:\n",
    "            print('Saving')\n",
    "            best_acc = epoch_acc\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(torch.load(SAVE_PATH))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs = 100, do_baseline = True):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    print('          ', end = '\\r')\n",
    "    acc = {'train':0.0, 'val':0.0}\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    if do_baseline:\n",
    "        acc['val'] = check_accuracy(model, phase = 'val', record_grad = False)\n",
    "        acc['train'] = check_accuracy(model, phase = 'train', record_grad = False)\n",
    "        print('.......... Baseline Evaluation Done ..............')\n",
    "        best_acc = acc['val']\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'val':\n",
    "                epoch_acc = check_accuracy(model, phase=phase, record_grad=False, criterion=criterion, optimizer=optimizer)\n",
    "                if epoch_acc > best_acc:\n",
    "                    print('Saving')\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), SAVE_PATH)\n",
    "            else:\n",
    "                epoch_acc, epoch_loss = check_accuracy(model, phase=phase, record_grad=True, criterion=criterion, optimizer=optimizer)\n",
    "   \n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(torch.load(SAVE_PATH))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "model = AlexNet(init_model=alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19086, 40000, 47.72%\n",
      "train Acc: 47.7150 %\n",
      "Total time taken = 94.63574719429016 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4772, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check_accuracy(model, 'train', record_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune_kwargs = [\n",
    "#     [model.features[0], 'weight', 0.1],\n",
    "#     [model.features[3], 'weight', 0.1],\n",
    "#     [model.features[6], 'weight', 0.1],\n",
    "#     [model.features[8], 'weight', 0.1],\n",
    "#     [model.features[10], 'weight', 0.1],\n",
    "    \n",
    "#     [model.classifier[1], 'weight', 0.1],\n",
    "#     [model.classifier[4], 'weight', 0.1],\n",
    "#     [model.classifier[6], 'weight', 0.1]\n",
    "# ]\n",
    "\n",
    "# for kwarg in prune_kwargs:\n",
    "#     prune.l1_unstructured(kwarg[0], name = kwarg[1], amount = kwarg[2])\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[3], 'weight'),\n",
    "    (model.features[6], 'weight'),\n",
    "    (model.features[8], 'weight'),\n",
    "    (model.features[10], 'weight'),\n",
    "    (model.classifier[1], 'weight'),\n",
    "    (model.classifier[4], 'weight'),\n",
    "    (model.classifier[6], 'weight'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.6\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(SAVE_PATH))\n",
    "model.to(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18881, 40000, 47.20%\n",
      "train Acc: 47.2025 %\n",
      "Total time taken = 95.20125913619995 seconds\n",
      ".......... Baseline Evaluation Done ..............\n",
      "Epoch 0/99\n",
      "----------\n",
      "None\n",
      "161, 256, 62.89%, 1.72, 439.89None\n",
      "332, 512, 64.84%, 1.52, 828.06None\n",
      "512, 768, 66.67%, 1.37, 1178.78None\n",
      "685, 1024, 66.89%, 1.41, 1539.06None\n",
      "836, 1280, 65.31%, 1.98, 2046.29None\n",
      "942, 1536, 61.33%, 2.50, 2687.15None\n",
      "1057, 1792, 58.98%, 2.54, 3337.93None\n",
      "1184, 2048, 57.81%, 2.39, 3949.91None\n",
      "1312, 2304, 56.94%, 2.25, 4526.05None\n",
      "1403, 2560, 54.80%, 3.06, 5308.31None\n",
      "1512, 2816, 53.69%, 2.66, 5989.18None\n",
      "1652, 3072, 53.78%, 1.81, 6451.60None\n",
      "1789, 3328, 53.76%, 2.06, 6979.79None\n",
      "1975, 3584, 55.11%, 1.29, 7310.34None\n",
      "2169, 3840, 56.48%, 1.21, 7620.66None\n",
      "2336, 4096, 57.03%, 1.57, 8021.49None\n",
      "2481, 4352, 57.01%, 2.16, 8573.87None\n",
      "2617, 4608, 56.79%, 2.20, 9136.01None\n",
      "2745, 4864, 56.44%, 2.33, 9733.51None\n",
      "2855, 5120, 55.76%, 2.73, 10431.38None\n",
      "3021, 5376, 56.19%, 1.53, 10822.43None\n",
      "3203, 5632, 56.87%, 1.32, 11160.10None\n",
      "3377, 5888, 57.35%, 1.43, 11525.29None\n",
      "3519, 6144, 57.28%, 1.99, 12034.30None\n",
      "3636, 6400, 56.81%, 2.28, 12617.17None\n",
      "3741, 6656, 56.20%, 2.54, 13268.44None\n",
      "3832, 6912, 55.44%, 2.52, 13913.32None\n",
      "3955, 7168, 55.18%, 2.46, 14543.42None\n",
      "4072, 7424, 54.85%, 2.32, 15138.31None\n",
      "4189, 7680, 54.54%, 2.39, 15750.39None\n",
      "4304, 7936, 54.23%, 2.29, 16336.10None\n",
      "4415, 8192, 53.89%, 2.28, 16918.86None\n",
      "4525, 8448, 53.56%, 2.43, 17540.57None\n",
      "4665, 8704, 53.60%, 2.05, 18065.56None\n",
      "4801, 8960, 53.58%, 2.16, 18619.46None\n",
      "4918, 9216, 53.36%, 2.33, 19216.51None\n",
      "5035, 9472, 53.16%, 2.05, 19741.87None\n",
      "5143, 9728, 52.87%, 2.14, 20289.17None\n",
      "5263, 9984, 52.71%, 2.11, 20828.70None\n",
      "5401, 10240, 52.74%, 1.67, 21257.30None\n",
      "5554, 10496, 52.92%, 1.81, 21720.42None\n",
      "5665, 10752, 52.69%, 2.43, 22341.78None\n",
      "5792, 11008, 52.62%, 2.09, 22876.49None\n",
      "5940, 11264, 52.73%, 1.68, 23305.73None\n",
      "6073, 11520, 52.72%, 2.12, 23849.00None\n",
      "6254, 11776, 53.11%, 1.18, 24150.54None\n",
      "6401, 12032, 53.20%, 2.06, 24677.17None\n",
      "6534, 12288, 53.17%, 1.92, 25167.53None\n",
      "6657, 12544, 53.07%, 2.17, 25722.92None\n",
      "6783, 12800, 52.99%, 2.28, 26305.49None\n",
      "6974, 13056, 53.42%, 1.09, 26583.61None\n",
      "7113, 13312, 53.43%, 2.10, 27121.39None\n",
      "7272, 13568, 53.60%, 1.78, 27576.25None\n",
      "7422, 13824, 53.69%, 1.90, 28063.55None\n",
      "7557, 14080, 53.67%, 1.96, 28565.09None\n",
      "7677, 14336, 53.55%, 2.32, 29158.89None\n",
      "7823, 14592, 53.61%, 2.03, 29677.34None\n",
      "7951, 14848, 53.55%, 2.37, 30282.85None\n",
      "8091, 15104, 53.57%, 2.18, 30841.84None\n",
      "8206, 15360, 53.42%, 2.36, 31446.72None\n",
      "8350, 15616, 53.47%, 1.90, 31933.81None\n",
      "8487, 15872, 53.47%, 2.13, 32480.23None\n",
      "8609, 16128, 53.38%, 2.59, 33143.49None\n",
      "8752, 16384, 53.42%, 1.90, 33629.69None\n",
      "8833, 16640, 53.08%, 3.37, 34492.62None\n",
      "8934, 16896, 52.88%, 2.81, 35213.26None\n",
      "9046, 17152, 52.74%, 2.83, 35938.10None\n",
      "9150, 17408, 52.56%, 2.87, 36672.09None\n",
      "9248, 17664, 52.36%, 2.92, 37419.86None\n",
      "9335, 17920, 52.09%, 3.05, 38201.13None\n",
      "9456, 18176, 52.02%, 2.43, 38823.73None\n",
      "9543, 18432, 51.77%, 2.97, 39585.18None\n",
      "9633, 18688, 51.55%, 3.34, 40439.00None\n",
      "9749, 18944, 51.46%, 2.73, 41137.33None\n",
      "9862, 19200, 51.36%, 2.64, 41813.45None\n",
      "9967, 19456, 51.23%, 2.54, 42464.52None\n",
      "10077, 19712, 51.12%, 2.99, 43230.61None\n",
      "10180, 19968, 50.98%, 2.89, 43970.51None\n",
      "10251, 20224, 50.69%, 3.60, 44892.27None\n",
      "10350, 20480, 50.54%, 2.51, 45536.05None\n",
      "10442, 20736, 50.36%, 3.30, 46381.69None\n",
      "10551, 20992, 50.26%, 2.92, 47130.22None\n",
      "10658, 21248, 50.16%, 2.73, 47828.90None\n",
      "10780, 21504, 50.13%, 2.42, 48449.49None\n",
      "10872, 21760, 49.96%, 3.14, 49254.41None\n",
      "11000, 22016, 49.96%, 2.46, 49882.99None\n",
      "11117, 22272, 49.91%, 2.72, 50580.28None\n",
      "11239, 22528, 49.89%, 2.82, 51302.11None\n",
      "11385, 22784, 49.97%, 2.22, 51869.16None\n",
      "11517, 23040, 49.99%, 2.48, 52504.49None\n",
      "11637, 23296, 49.95%, 2.40, 53118.11None\n",
      "11719, 23552, 49.76%, 3.50, 54015.10None\n",
      "11818, 23808, 49.64%, 3.07, 54800.27None\n",
      "11905, 24064, 49.47%, 3.33, 55653.60None\n",
      "12044, 24320, 49.52%, 2.27, 56234.57None\n",
      "12177, 24576, 49.55%, 2.35, 56837.02None\n",
      "12275, 24832, 49.43%, 3.23, 57663.44None\n",
      "12373, 25088, 49.32%, 3.11, 58460.87None\n",
      "12480, 25344, 49.24%, 2.91, 59206.80None\n",
      "12558, 25600, 49.05%, 3.02, 59979.27None\n",
      "12691, 25856, 49.08%, 2.47, 60611.53None\n",
      "12780, 26112, 48.94%, 3.26, 61447.19None\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-10, momentum = 0.9)\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimzer_ft, step_size = 7, gamma = 0.1)\n",
    "\n",
    "model = train_limited(model, criterion, optimizer, do_baseline = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in list(model.features):\n",
    "    if prune.is_pruned(module):\n",
    "        print(module)\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "for module in list(model.classifier):\n",
    "    if prune.is_pruned(module):\n",
    "        print(module)\n",
    "        prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(model, 'train', record_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'D://models//undone_pruned_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(SAVE_PATH)) \n",
    "# model.to(device)\n",
    "# torch.cuda.empty_cache()\n",
    "# check_accuracy(model, 'train', record_grad = False)\n",
    "\n",
    "# for lr in [10000, 1000, 100, 10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]:\n",
    "#     model.load_state_dict(torch.load(SAVE_PATH)) \n",
    "#     model.to(device)\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    \n",
    "#     print('--------------------')\n",
    "#     print('lr = {}'.format(lr))\n",
    "#     print()\n",
    "    \n",
    "#     check_accuracy(model, 'train', record_grad = True, criterion = criterion, optimizer = optimizer)\n",
    "#     check_accuracy(model, 'train', record_grad = True, criterion = criterion, optimizer = optimizer)\n",
    "#     check_accuracy(model, 'train', record_grad = True, criterion = criterion, optimizer = optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
