{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements 1 level cache (No backward pass needed so not implemented; only accuracy determination required)\n",
    "# Weight sharing not included\n",
    "\n",
    "# Cache.approx just updates W, A values according to hits and misses and does not compute the result. Actual result computation\n",
    "# is done at the end in parallel by doing updated_W * updated_A\n",
    "# Current - LRU policy replacement. Future - May use LRU + count-used based policy\n",
    "\n",
    "# Find out how hdw simulation is done in papers. They ofcourse don't actually build the hdw, they just simulate it. But, \n",
    "# they are still able to find the accuracy value by running the program over their hdw simulation\n",
    "\n",
    "# To be added (and be performed in this order):\n",
    "# Network Pruning\n",
    "# Wt sharing\n",
    "# Bit Masking (both weights and activations; may try retraining also for both) / Pytorch trained 8 bit int quantization\n",
    "# May merge weight bit masking in 3rd step with 2nd step \n",
    "\n",
    "# Find out why hit rate is much higher than before. Check if cache is working correctly by using custom examples\n",
    "\n",
    "# Find out exactly how 1000 caches will be used in hdw. They all have the same data ? If miss occurs in one cache, access \n",
    "# to all caches will be stopped ? Using the parallelism of 1000 caches to process misses in parallel can really speed up\n",
    "# the simulation. The sequential processing of misses is the current bottleneck\n",
    "\n",
    "# Speed Up methods:\n",
    "# Prune W and A in initial step to be able to increase batch size (Currently, very difficult to vectorize. Basically, hard\n",
    "# to get equal size search ranges for each unique weight in the new weight-activation pairs to prune cache_W. Search ranges\n",
    "# can be made equal for each unique weight in a sequential manner)\n",
    "# Use multiple caches which are dependant / independent\n",
    "# May do distance calculation between new weight-activation pairs and cache pairs in batches to increase the batch size fed to\n",
    "# the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import  torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from statistics import mean\n",
    "from collections  import OrderedDict\n",
    "from collections  import namedtuple\n",
    "import sys\n",
    "\n",
    "device = torch.device('cuda')\n",
    "SAVE_PATH = 'D://models//main_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:\\\\datasets\\\\ILSVRC2012_img_val - Retrain\\\\'\n",
    "dataset = {x:datasets.ImageFolder(os.path.join(data_dir, x), transform[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {x:torch.utils.data.DataLoader(dataset[x], batch_size = 1, shuffle = False)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {x:len(dataset[x]) for x in ['train', 'val']}\n",
    "class_names = dataset['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cache:\n",
    "    global device\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.size = 10 ** 4\n",
    "        self.W = torch.randn(self.size, device = device)\n",
    "        self.A = torch.randn(self.size, device = device)\n",
    "        self.time = torch.ones(self.size, device = device, dtype = torch.int32)\n",
    "        self.epsilon = 1e-2\n",
    "        self.hits = torch.tensor(0, device = device, dtype = torch.int64).view(1)\n",
    "        self.misses = torch.tensor(0, device = device, dtype = torch.int64).view(1)\n",
    "        \n",
    "    def batch_wise_approx(self, orig_W, orig_A, num_send):\n",
    "        \n",
    "        if orig_W.shape != orig_A.shape:\n",
    "            sys.exit(\"W-shape and A-shape unequal\")\n",
    "        \n",
    "        shape = orig_W.shape\n",
    "        num_elem = orig_W.numel()\n",
    "        \n",
    "        orig_W = orig_W.flatten()\n",
    "        orig_A = orig_A.flatten()\n",
    "        \n",
    "        out_W = torch.zeros(orig_W.shape, dtype = torch.float32, device = device)\n",
    "        out_A = torch.zeros(orig_A.shape, dtype = torch.float32, device = device)\n",
    "        \n",
    "        for i in range(int(num_elem / num_send) + 1):\n",
    "            start = i * num_send\n",
    "            end = min((i + 1) * num_send, num_elem)\n",
    "            out_W[start:end], out_A[start:end], hits, misses = self.approx(orig_W[start:end], orig_A[start:end])\n",
    "            tot = hits + misses\n",
    "            print('\\r{} / {} | Hits {} / {}, {:.2f}%'.format(i, int(num_elem / num_send), hits, tot,  hits * 100.0 / tot), end = '')\n",
    "            \n",
    "        return out_W.view(shape), out_A.view(shape)\n",
    "        \n",
    "    def approx(self, orig_W, orig_A):\n",
    "        \n",
    "        s1 = orig_W.shape\n",
    "        s2 = orig_A.shape\n",
    "        \n",
    "        orig_W = orig_W.flatten()\n",
    "        orig_A = orig_A.flatten()\n",
    "        if orig_W.shape != orig_A.shape:\n",
    "            sys.exit(\"W-shape and A-shape unequal\")\n",
    "        \n",
    "        # Remove indices where W = 0 or A = 0\n",
    "        non_zero_mask = ~((orig_W == 0) | (orig_A == 0))\n",
    "        W_ = orig_W[non_zero_mask]\n",
    "        A_ = orig_A[non_zero_mask]\n",
    "        \n",
    "        W = W_\n",
    "        A = A_\n",
    "        \n",
    "        first_time = True\n",
    "        while(True):\n",
    "            \n",
    "            #print('\\r Size is: {}, {}'.format(len(W), len(A)), end = '')\n",
    "            \n",
    "            # Find hits, misses\n",
    "            if first_time:\n",
    "                I = torch.cat((W.view(1,-1),A.view(1,-1)), dim = 0)\n",
    "                S = torch.cat((self.W.view(1,-1), self.A.view(1,-1)), dim = 0)\n",
    "                x = I.expand(S.shape[1],-1,-1)\n",
    "                y = S.t().view(S.shape[1],2,1).expand(-1,-1,I.shape[1])\n",
    "                dist, LI_orig = torch.abs(x - y).sum(dim = 1).min(dim = 0)\n",
    "                first_time = False\n",
    "            else:\n",
    "                # Calculate distances of the elements from newly added cache pair\n",
    "                I = torch.cat((W.view(1,-1),A.view(1,-1)), dim = 0)\n",
    "                S = buffer\n",
    "                x = I.expand(S.shape[1],-1,-1)\n",
    "                y = S.t().view(S.shape[1],2,1).expand(-1,-1,I.shape[1])\n",
    "                dist_t = torch.abs(x - y).sum(dim = 1).squeeze(0)\n",
    "#                 print('dist', dist)\n",
    "#                 print('dist_t', dist_t)\n",
    "                \n",
    "                # Check if newly added cache pair better approximates some elements\n",
    "                change = dist_t < dist\n",
    "                change_t = ~ change\n",
    "                LI_orig = change_t * LI_orig + change * buf_ind.expand_as(change)\n",
    "                dist = change_t * dist + change * dist_t\n",
    "                \n",
    "#                 print('change', change)\n",
    "#                 print('dist', dist)\n",
    "                \n",
    "                # Recalculation for elements that matched with removed cache pair\n",
    "                incorrect = (LI_orig == buf_ind) & change_t\n",
    "#                 print(incorrect)\n",
    "                if incorrect.any():\n",
    "                    W_i = W[incorrect]\n",
    "                    A_i = A[incorrect]\n",
    "                    I = torch.cat((W_i.view(1,-1),A_i.view(1,-1)), dim = 0)\n",
    "                    S = torch.cat((self.W.view(1,-1), self.A.view(1,-1)), dim = 0)\n",
    "                    x = I.expand(S.shape[1],-1,-1)\n",
    "                    y = S.t().view(S.shape[1],2,1).expand(-1,-1,I.shape[1])\n",
    "                    dist[incorrect], LI_orig[incorrect] = torch.abs(x - y).sum(dim = 1).min(dim = 0)\n",
    "                \n",
    "\n",
    "            misses = ~(dist < self.epsilon)\n",
    "            t = misses.nonzero()\n",
    "            \n",
    "            if t.shape[0] != 0:\n",
    "                max_lim = t[0]\n",
    "            else:\n",
    "                max_lim = W.shape[0]\n",
    "            LI = LI_orig[:max_lim]\n",
    "            \n",
    "            if LI.shape[0] != 0: # Hits encountered\n",
    "                \n",
    "                # Update num hits\n",
    "                self.hits += max_lim\n",
    "               \n",
    "                # Update W, A corresponding to hits\n",
    "                W[:max_lim] = self.W[LI]\n",
    "                A[:max_lim] = self.A[LI]\n",
    "\n",
    "                # Update time corresponding to hits\n",
    "\n",
    "                used = torch.unique(LI)\n",
    "                x = LI.view(-1,1) == used.view(1,-1)\n",
    "                x = x.type(torch.int32)\n",
    "                x = x * torch.arange(start = 1, end = x.shape[0]+1, device = device).view(-1,1).expand_as(x)\n",
    "                x = x.shape[0] - 1 - torch.argmax(x, dim = 0)\n",
    "\n",
    "                self.time += LI.shape[0]\n",
    "                self.time[used] = 0\n",
    "                self.time[used] += x\n",
    "\n",
    "#                 print('Time', self.time)\n",
    "#                 print('W', W)\n",
    "#                 print('A', A)\n",
    "            \n",
    "            if t.shape[0] != 0: # A miss encountered\n",
    "            \n",
    "                self.misses += 1\n",
    "                \n",
    "                # Update stored values in cache\n",
    "                ind = torch.argmax(self.time)\n",
    "                self.W[ind] = W[max_lim]\n",
    "                self.A[ind] = A[max_lim]\n",
    "                self.time += 1\n",
    "                self.time[ind] = 0\n",
    "                \n",
    "#                 print('Miss at', max_lim, W[max_lim], A[max_lim])\n",
    "#                 print('New cache W', self.W)\n",
    "#                 print('New cache A', self.A)\n",
    "#                 print('New time', self.time)\n",
    "                \n",
    "                if max_lim == W.shape[0] - 1:\n",
    "                    break\n",
    "                \n",
    "                # Update W, A, for next cycle\n",
    "                buffer = torch.cat((W[max_lim].view(1,-1),A[max_lim].view(1,-1)), dim = 0)\n",
    "                buf_ind = ind\n",
    "                W = W[max_lim + 1:]\n",
    "                A = A[max_lim + 1:]\n",
    "                dist = dist[max_lim + 1:]\n",
    "                LI_orig = LI_orig[max_lim + 1:]\n",
    "                \n",
    "#                 print('W', W)\n",
    "#                 print('A', A)\n",
    "#                 print()\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        orig_W[non_zero_mask] = W_\n",
    "        orig_A[non_zero_mask] = A_\n",
    "        \n",
    "        hits = self.hits.item()\n",
    "        misses = self.misses.item()\n",
    "        \n",
    "        self.hits = torch.tensor(0, device = device, dtype = torch.int64).view(1)\n",
    "        self.misses = torch.tensor(0, device = device, dtype = torch.int64).view(1)\n",
    "\n",
    "        return orig_W.view(s1), orig_A.view(s2), hits, misses\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_cache = cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing\n",
    "\n",
    "# global_cache.size = 3\n",
    "# global_cache.epsilon = 3\n",
    "# global_cache.W = torch.tensor([1.,2,3], device = device, dtype = torch.float32)\n",
    "# global_cache.A = torch.tensor([7.,2,5], device = device, dtype = torch.float32)\n",
    "# global_cache.time = torch.ones(global_cache.size, device = device, dtype = torch.int32)\n",
    "\n",
    "# x = torch.tensor([1., 2, 3, 1, 2, 1, 1, 3, 0, 3, 2], device = device, dtype = torch.float32)\n",
    "# y = torch.tensor([1., 2, 3, 4, 5, 6, 7, 8, 9, 10, 0], device = device, dtype = torch.float32)\n",
    "# x, y, hits, misses = global_cache.approx(x, y)\n",
    "\n",
    "# print(x)\n",
    "# print(y)\n",
    "# print(hits, misses)\n",
    "\n",
    "# # Expected Answer\n",
    "# # tensor([2., 2., 2., 1., 1., 1., 1., 3., 0., 3., 2.], device='cuda:0')\n",
    "# # tensor([2., 2., 2., 4., 4., 7., 7., 8., 9., 8., 0.], device='cuda:0')\n",
    "# # 7.0 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing\n",
    "\n",
    "# global_cache.size = 1\n",
    "# global_cache.epsilon = 0.1\n",
    "# global_cache.W = torch.tensor([1.], device = device, dtype = torch.float32)\n",
    "# global_cache.A = torch.tensor([7.], device = device, dtype = torch.float32)\n",
    "# global_cache.time = torch.ones(global_cache.size, device = device, dtype = torch.int32)\n",
    "\n",
    "# x = torch.tensor([1., 2, 3, 1, 2, 1, 1, 3, 0, 3, 2], device = device, dtype = torch.float32)\n",
    "# y = torch.tensor([1., 2, 3, 4, 5, 6, 7, 8, 9, 10, 0], device = device, dtype = torch.float32)\n",
    "# x, y, hits, misses = global_cache.approx(x, y)\n",
    "\n",
    "# print(x)\n",
    "# print(y)\n",
    "# print(hits, misses)\n",
    "\n",
    "# # Expected Answer\n",
    "# # tensor([1., 2., 3., 1., 2., 1., 1., 3., 0., 3., 2.], device='cuda:0')\n",
    "# # tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.,  0.],\n",
    "# #        device='cuda:0')\n",
    "# # 0 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cached_conv(nn.Module):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    def __init__(self, wt_layer, cache):\n",
    "        super(cached_conv, self).__init__()\n",
    "        self.weight = wt_layer.weight\n",
    "        self.bias = wt_layer.bias\n",
    "        self.stride = wt_layer.stride\n",
    "        self.padding = wt_layer.padding\n",
    "        #self.dilation = wt_layer.dilation\n",
    "        #self.groups = wt_layer.groups\n",
    "        self.cache = cache\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print('Reached conv')\n",
    "        \n",
    "        A_prev = x\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        stride = self.stride\n",
    "        pad = self.padding\n",
    "        cache = self.cache\n",
    "        \n",
    "        #return F.conv2d(x, W, bias = b, stride = stride, padding = pad)\n",
    "        \n",
    "        (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
    "        (n_C, n_C_prev, f, f) = W.shape\n",
    "        \n",
    "        # Compute the dimensions of the CONV output volume \n",
    "        n_H = int((n_H_prev + 2*pad[0] - f)/stride[0]) + 1\n",
    "        n_W =int((n_W_prev + 2*pad[1] - f)/stride[1]) + 1\n",
    "\n",
    "        y = F.unfold(A_prev, (f, f), padding = pad, stride = stride).transpose(2,1)\n",
    "        #y = y.view(m, 1, y.shape[1],y.shape[2]).repeat((1,n_C,1,1))\n",
    "        y = y.view(m, 1, y.shape[1],y.shape[2]).expand((-1,n_C,-1,-1))\n",
    "\n",
    "        W = W.view(n_C, -1)\n",
    "        #W = W.view(1,n_C, 1, W.shape[1]).repeat(m, 1, y.shape[2], 1)\n",
    "        W = W.view(1,n_C, 1, W.shape[1]).expand(m, -1, y.shape[2], -1)\n",
    "        \n",
    "        W, y = cache.batch_wise_approx(W, y, 10000)\n",
    "        \n",
    "        Z = torch.sum(W * y, dim = 3).view(m, n_C, n_H, n_W)\n",
    "        Z = Z + b.view(1,b.shape[0], 1, 1)\n",
    "        \n",
    "        #print(torch.sum(torch.abs(Z - F.conv2d(x, self.weight, bias = self.bias, stride = self.stride, padding = self.padding))))\n",
    "        \n",
    "        #sys.exit()\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cached_fc(nn.Module):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    def __init__(self, wt_layer, cache):\n",
    "        super(cached_fc, self).__init__()\n",
    "        self.weight = wt_layer.weight\n",
    "        self.bias = wt_layer.bias\n",
    "        self.cache = cache\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print('Reached fc')\n",
    "               \n",
    "        return F.linear(x, self.weight, bias = self.bias)\n",
    "        \n",
    "        A_prev = x\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        cache = self.cache\n",
    "        \n",
    "        (m, n_prev) = A_prev.shape\n",
    "        (n, n_prev) = W.shape\n",
    "\n",
    "        A_prev = A_prev.view(m, 1, n_prev).expand(-1, n, -1)\n",
    "        W = W.view(1, n, n_prev).expand(m, -1, -1)\n",
    "\n",
    "        W, A_prev = cache.batch_wise_approx(W, A_prev, 10000)\n",
    "        \n",
    "        Z = (A_prev * W).sum(dim = 2).view(m, n)\n",
    "        Z = Z + b.view(1, n)\n",
    "        \n",
    "        #print(torch.sum(torch.abs(Z - F.linear(x, self.weight, bias = self.bias))))\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, init_state_dict, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        self.load_state_dict(init_state_dict)\n",
    "        \n",
    "        self.init_cache_layers()\n",
    "       \n",
    "    def init_cache_layers(self):\n",
    "        \n",
    "        ind = -1\n",
    "        global global_cache \n",
    "        q_list = []\n",
    "        for layer in self.features:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                ind += 1\n",
    "                q_list.append(cached_conv(layer, global_cache))\n",
    "            else:\n",
    "                q_list.append(layer)\n",
    "        self.features = nn.Sequential(*q_list)\n",
    "        \n",
    "        ind = -1\n",
    "        q_list = []\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                ind += 1\n",
    "                q_list.append(cached_fc(layer, global_cache))\n",
    "            else:\n",
    "                q_list.append(layer)\n",
    "        self.classifier = nn.Sequential(*q_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, phase):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "#     if record_grad:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "\n",
    "        \n",
    "    done = 0\n",
    "    acc = 0.0\n",
    "    since = time.time()\n",
    "    corrects = torch.tensor(0)\n",
    "    total_loss = 0.0\n",
    "    corrects = corrects.to(device)\n",
    "    loss = 100.0\n",
    "    \n",
    "    for inputs, labels in dataloader[phase]:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            corrects += torch.sum(preds == labels)\n",
    "\n",
    "        done += len(inputs)\n",
    "        print('\\r{}, {}, {:.2f}%, {:.2f}'.format(corrects.item(), done, corrects.item() * 100.0 / done, total_loss), end = '')\n",
    "#         if done >= 1000:\n",
    "#             break\n",
    "                    \n",
    "    acc = corrects.double() / done\n",
    "    print('\\n{} Acc: {:.4f} %'.format(phase, acc * 100))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Total time taken = {} seconds'.format(time_elapsed))\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "model = AlexNet(init_state_dict=torch.load('D://models//undone_wt_shared_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5, inplace=False)\n",
       "  (1): cached_fc()\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): cached_fc()\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): cached_fc()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached conv\n",
      "558 / 7027 | Hits 7524 / 7524, 100.00%"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "check_accuracy(model, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_cache.misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
