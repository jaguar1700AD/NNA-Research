{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Left - Remove the pruning layers as if they never existed and save the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import  torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from statistics import mean\n",
    "from collections  import OrderedDict\n",
    "from collections  import namedtuple\n",
    "import sys\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "device = torch.device('cuda')\n",
    "SAVE_PATH = '../models/Pruned_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../datasets/ILSVRC2012_img_val - Retrain/'\n",
    "dataset = {x:datasets.ImageFolder(os.path.join(data_dir, x), transform[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {x:torch.utils.data.DataLoader(dataset[x], batch_size = 512, shuffle = False, num_workers = 16, pin_memory = True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {x:len(dataset[x]) for x in ['train', 'val']}\n",
    "class_names = dataset['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, init_model, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        self.load_state_dict(copy.deepcopy(init_model.state_dict()))\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, phase, record_grad, criterion = None, optimizer = None):\n",
    "    \n",
    "    global device\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "#     if record_grad:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "\n",
    "        \n",
    "    done = 0\n",
    "    acc = 0.0\n",
    "    since = time.time()\n",
    "    corrects = torch.tensor(0)\n",
    "    total_loss = 0.0\n",
    "    corrects = corrects.to(device)\n",
    "    loss = 100.0\n",
    "    \n",
    "    for inputs, labels in dataloader[phase]:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if record_grad:\n",
    "            with torch.set_grad_enabled(True):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects += torch.sum(preds == labels)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                #print(model.features[0].weight_orig.grad)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            done += len(inputs)\n",
    "            print('\\r{}, {}, {:.2f}%, {:.2f}, {:.2f}'.format(corrects.item(), done, corrects.item() * 100.0 / done, loss.item(), total_loss), end = '')\n",
    "\n",
    "        else:\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects += torch.sum(preds == labels)\n",
    "\n",
    "            done += len(inputs)\n",
    "            print('\\r{}, {}, {:.2f}%'.format(corrects.item(), done, corrects.item() * 100.0 / done), end = '')\n",
    "\n",
    "    acc = corrects.double() / done\n",
    "    print('\\n{} Acc: {:.4f} %'.format(phase, acc * 100))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Total time taken = {} seconds'.format(time_elapsed))\n",
    "\n",
    "    if record_grad:\n",
    "        return acc, total_loss\n",
    "    else:\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_limited(model, criterion, optimizer, num_epochs = 100, do_baseline = True):\n",
    "    \n",
    "    global device\n",
    "    global SAVE_PATH\n",
    "    \n",
    "    print('          ', end = '\\r')\n",
    "    acc = {'train':0.0, 'val':0.0}\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    if do_baseline:\n",
    "        acc['train'] = check_accuracy(model, phase = 'train', record_grad = False)\n",
    "        print('.......... Baseline Evaluation Done ..............')\n",
    "        best_acc = acc['train']\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'val':\n",
    "                epoch_acc = check_accuracy(model, phase='train', record_grad=False, criterion=criterion, optimizer=optimizer)\n",
    "                if epoch_acc > best_acc:\n",
    "                    print('Saving')\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), SAVE_PATH)\n",
    "            else:\n",
    "                epoch_acc, epoch_loss = check_accuracy(model, phase='train', record_grad=True, criterion=criterion, optimizer=optimizer)\n",
    "        print()\n",
    "                                                       \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(torch.load(SAVE_PATH))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs = 100, do_baseline = True):\n",
    "    \n",
    "    global device\n",
    "    global SAVE_PATH\n",
    "    \n",
    "    print('          ', end = '\\r')\n",
    "    acc = {'train':0.0, 'val':0.0}\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    if do_baseline:\n",
    "        acc['val'] = check_accuracy(model, phase = 'val', record_grad = False)\n",
    "        acc['train'] = check_accuracy(model, phase = 'train', record_grad = False)\n",
    "        print('.......... Baseline Evaluation Done ..............')\n",
    "        best_acc = acc['val']\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'val':\n",
    "                epoch_acc = check_accuracy(model, phase=phase, record_grad=False, criterion=criterion, optimizer=optimizer)\n",
    "                if epoch_acc > best_acc:\n",
    "                    print('Saving')\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), SAVE_PATH)\n",
    "            else:\n",
    "                epoch_acc, epoch_loss = check_accuracy(model, phase=phase, record_grad=True, criterion=criterion, optimizer=optimizer)\n",
    "   \n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(torch.load(SAVE_PATH))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "model = AlexNet(init_model=alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19061, 40000, 47.65%\n",
      "train Acc: 47.6525 %\n",
      "Total time taken = 25.43606472015381 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4765, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy(model, 'train', record_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune_kwargs = [\n",
    "#     [model.features[0], 'weight', 0.1],\n",
    "#     [model.features[3], 'weight', 0.1],\n",
    "#     [model.features[6], 'weight', 0.1],\n",
    "#     [model.features[8], 'weight', 0.1],\n",
    "#     [model.features[10], 'weight', 0.1],\n",
    "    \n",
    "#     [model.classifier[1], 'weight', 0.1],\n",
    "#     [model.classifier[4], 'weight', 0.1],\n",
    "#     [model.classifier[6], 'weight', 0.1]\n",
    "# ]\n",
    "\n",
    "# for kwarg in prune_kwargs:\n",
    "#     prune.l1_unstructured(kwarg[0], name = kwarg[1], amount = kwarg[2])\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[3], 'weight'),\n",
    "    (model.features[6], 'weight'),\n",
    "    (model.features[8], 'weight'),\n",
    "    (model.features[10], 'weight'),\n",
    "    (model.classifier[1], 'weight'),\n",
    "    (model.classifier[4], 'weight'),\n",
    "    (model.classifier[6], 'weight'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.6\n",
    ")\n",
    "\n",
    "# torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(SAVE_PATH))\n",
    "model.to(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19030, 40000, 47.58%\n",
      "train Acc: 47.5750 %\n",
      "Total time taken = 25.385926723480225 seconds\n",
      ".......... Baseline Evaluation Done ..............\n",
      "Epoch 0/99\n",
      "----------\n",
      "19027, 40000, 47.57%, 4.00, 97315.43\n",
      "train Acc: 47.5675 %\n",
      "Total time taken = 28.43674874305725 seconds\n",
      "19141, 40000, 47.85%\n",
      "train Acc: 47.8525 %\n",
      "Total time taken = 24.995118618011475 seconds\n",
      "Saving\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "18984, 40000, 47.46%, 3.67, 97218.20\n",
      "train Acc: 47.4600 %\n",
      "Total time taken = 28.28479313850403 seconds\n",
      "19085, 40000, 47.71%\n",
      "train Acc: 47.7125 %\n",
      "Total time taken = 25.609869480133057 seconds\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "4760, 8704, 54.69%, 2.16, 17475.56"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-94880cbdb8df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#exp_lr_scheduler = lr_scheduler.StepLR(optimzer_ft, step_size = 7, gamma = 0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_limited\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a2f2c1e0353e>\u001b[0m in \u001b[0;36mtrain_limited\u001b[0;34m(model, criterion, optimizer, num_epochs, do_baseline)\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mepoch_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ce927ec3b7d8>\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[0;34m(model, phase, record_grad, criterion, optimizer)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-8, momentum = 0.9)\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimzer_ft, step_size = 7, gamma = 0.1)\n",
    "\n",
    "model = train_limited(model, criterion, optimizer, do_baseline = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(SAVE_PATH))\n",
    "model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Linear(in_features=9216, out_features=4096, bias=True)\n",
      "Linear(in_features=4096, out_features=4096, bias=True)\n",
      "Linear(in_features=4096, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for module in list(model.features):\n",
    "    if prune.is_pruned(module):\n",
    "        print(module)\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "for module in list(model.classifier):\n",
    "    if prune.is_pruned(module):\n",
    "        print(module)\n",
    "        prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19078, 40000, 47.70%\n",
      "train Acc: 47.6950 %\n",
      "Total time taken = 24.81940460205078 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4770, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy(model, 'train', record_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/undone_pruned_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(SAVE_PATH)) \n",
    "# model.to(device)\n",
    "# torch.cuda.empty_cache()\n",
    "# check_accuracy(model, 'train', record_grad = False)\n",
    "\n",
    "# for lr in [10000, 1000, 100, 10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]:\n",
    "#     model.load_state_dict(torch.load(SAVE_PATH)) \n",
    "#     model.to(device)\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    \n",
    "#     print('--------------------')\n",
    "#     print('lr = {}'.format(lr))\n",
    "#     print()\n",
    "    \n",
    "#     check_accuracy(model, 'train', record_grad = True, criterion = criterion, optimizer = optimizer)\n",
    "#     check_accuracy(model, 'train', record_grad = True, criterion = criterion, optimizer = optimizer)\n",
    "#     check_accuracy(model, 'train', record_grad = True, criterion = criterion, optimizer = optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
